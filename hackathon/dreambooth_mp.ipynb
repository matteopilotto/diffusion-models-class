{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b69b39-ca4c-4650-a042-fbba2a578094",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071cae0-c686-4949-849c-b0185db1df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq diffusers transformers datasets accelerate ftfy bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f870-b181-4da6-8f95-d9139d4f8198",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220c1b84-0fab-44c3-bff3-549333d2a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatt24\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel\n",
    "from argparse import Namespace\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a28cb24-b187-4fb4-a75b-ca1dd316e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lewtun--corgi-387a0d84d49c152d\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/lewtun___parquet/lewtun--corgi-387a0d84d49c152d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'lewtun/corgi'\n",
    "dataset = load_dataset(dataset_name, split='train')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07594293-5dc3-47ac-8db1-0c232e34d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d55662-3ff7-4b6d-8388-15187107dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid([image.resize((256, 256)) for image in dataset['image']], rows=1, cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bced5e5-45e7-48f4-9398-30d741b37ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = 'ccorgi'\n",
    "class_type = 'dog'\n",
    "instance_prompt = f'a photo of {unique_id} {class_type}'\n",
    "print(instance_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb28a5-dd5b-4721-bc87-43f03b9d4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(self, dataset, instance_prompt, tokenizer, img_size=512):\n",
    "        self.dataset = dataset\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.img_size = img_size\n",
    "        \n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.CenterCrop(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        image = self.dataset[index]['image']\n",
    "        example['instance_image'] = self.img_transforms(image)\n",
    "        example['instance_input_ids'] = self.tokenizer(\n",
    "                                            self.instance_prompt,\n",
    "                                            padding='do_not_pad',\n",
    "                                            truncation=True,\n",
    "                                            max_length=self.tokenizer.model_max_length\n",
    "                                        ).input_ids\n",
    "        \n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af801071-0385-4cdd-a369-718425908a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "sd_ckpt = 'CompVis/stable-diffusion-v1-4'\n",
    "tokenizer = CLIPTokenizer.from_pretrained(sd_ckpt, subfolder='tokenizer')\n",
    "\n",
    "train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer, img_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280e9e1-def0-4d35-ac3b-ba06ad4da1ba",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991321ee-62a1-4ad8-aa87-8f93729b7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: List[Dict[str, Tensor]]) -> Dict[str, List[Tensor]]:\n",
    "    input_ids = [example['instance_input_ids'] for example in examples]\n",
    "    pixel_values = [example['instance_image'] for example in examples]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = tokenizer.pad({'input_ids': input_ids}, padding=True, return_tensors='pt').input_ids\n",
    "    \n",
    "    batch = {'input_ids': input_ids,\n",
    "             'pixel_values': pixel_values}\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc2961-8c5c-40eb-8567-8b5db72f9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [train_dataset[i] for i in range(len(train_dataset))]\n",
    "batch = collate_fn(examples)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720309a-93f4-40ea-bf87-6fdd0d84e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(sd_ckpt, subfolder='text_encoder')\n",
    "vae = AutoencoderKL.from_pretrained(sd_ckpt, subfolder='vae')\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_ckpt, subfolder='unet')\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained('openai/clip-vit-base-patch32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ce3f5-d65d-4681-82b9-4bbece2b3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dl = DataLoader(dataset=train_dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4bd841-4627-40e1-850a-63c7e169157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "# batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72e8f0-833c-4b8b-8aef-a5da9629cdec",
   "metadata": {},
   "source": [
    "### Fine-tuning with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556acb06-4aab-4a03-aff1-396bcecd98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-6\n",
    "max_train_steps = 400\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    pretrained_model_name_or_path=sd_ckpt,\n",
    "    resolution=512, # Reduce this if you want to save some memory\n",
    "    train_dataset=train_dataset,\n",
    "    instance_prompt=instance_prompt,\n",
    "    learning_rate=lr,\n",
    "    max_train_steps=max_train_steps,\n",
    "    train_batch_size=1,\n",
    "    gradient_accumulation_steps=1, # Increase this if you want to lower memory usage\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,  # set this to True to lower the memory usage.\n",
    "    use_8bit_adam=True,  # use 8bit optimizer from bitsandbytes\n",
    "    seed=2077,\n",
    "    eval_batch_size=4,\n",
    "    output_dir=\"my-dreambooth\", # where to save the pipeline\n",
    "    project_name='dreambooth-demo-2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106dfc9-95ad-4d2e-abd3-d47c11b47a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torch.nn.functional as F\n",
    "# from accelerate import Accelerator\n",
    "# from accelerate.utils import set_seed\n",
    "# from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline\n",
    "# from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "# import random\n",
    "# import os\n",
    "\n",
    "\n",
    "# def training_function(text_encoder, vae, unet):\n",
    "\n",
    "#     accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "#                               log_with=\"wandb\",\n",
    "#                               logging_dir=os.path.join(args.output_dir, 'logs'))\n",
    "\n",
    "#     set_seed(args.seed)\n",
    "\n",
    "#     if args.gradient_checkpointing:\n",
    "#         unet.enable_gradient_checkpointing()\n",
    "\n",
    "#     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "#     if args.use_8bit_adam:\n",
    "#         import bitsandbytes as bnb\n",
    "#         optimizer_class = bnb.optim.AdamW8bit\n",
    "#     else:\n",
    "#         optimizer_class = torch.optim.AdamW\n",
    "\n",
    "#     optimizer = optimizer_class(params=unet.parameters(), lr=args.learning_rate)\n",
    "\n",
    "#     noise_scheduler = DDPMScheduler(\n",
    "#         beta_start=0.00085,\n",
    "#         beta_end=0.012,\n",
    "#         beta_schedule=\"scaled_linear\",\n",
    "#         num_train_timesteps=1000,\n",
    "#     )\n",
    "    \n",
    "#     inference_scheduler = PNDMScheduler(\n",
    "#         beta_start=0.00085,\n",
    "#         beta_end=0.012,\n",
    "#         beta_schedule=\"scaled_linear\",\n",
    "#         skip_prk_steps=True,\n",
    "#         steps_offset=1,\n",
    "#     )\n",
    "\n",
    "#     train_dataloader = DataLoader(\n",
    "#         dataset=args.train_dataset,\n",
    "#         batch_size=args.train_batch_size,\n",
    "#         shuffle=True,\n",
    "#         collate_fn=collate_fn,\n",
    "#     )\n",
    "\n",
    "#     unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
    "\n",
    "#     # Move text_encode and vae to gpu\n",
    "#     text_encoder.to(accelerator.device)\n",
    "#     vae.to(accelerator.device)\n",
    "\n",
    "#     # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "#     num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "#     num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "#     # Train!\n",
    "#     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "    \n",
    "#     # init tracker\n",
    "#     if accelerator.is_main_process:\n",
    "#         accelerator.init_trackers(project_name=args.project_name, config=args)\n",
    "    \n",
    "#     # Only show the progress bar once on each machine.\n",
    "#     progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "#     progress_bar.set_description(\"Steps\")\n",
    "#     global_step = 0\n",
    "\n",
    "#     for epoch in range(num_train_epochs):\n",
    "#         unet.train()\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             with accelerator.accumulate(unet):\n",
    "#                 # Convert images to latent space\n",
    "#                 with torch.inference_mode():\n",
    "#                     latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
    "#                     latents = latents * 0.18215\n",
    "\n",
    "#                 # Sample noise that we'll add to the latents\n",
    "#                 noise = torch.randn_like(latents).to(accelerator.device)\n",
    "#                 batch_size = latents.shape[0]\n",
    "                \n",
    "#                 # Sample a random timestep for each image\n",
    "#                 max_train_ts = noise_scheduler.config.num_train_timesteps\n",
    "#                 timesteps = (torch.tensor([random.randint(0, max_train_ts-1) for _ in range(batch_size)])\n",
    "#                              .to(latents.device))\n",
    "\n",
    "#                 # Add noise to the latents according to the noise magnitude at each timestep\n",
    "#                 # (this is the forward diffusion process)\n",
    "#                 noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "#                 # Get the text embedding for conditioning\n",
    "#                 with torch.inference_mode():\n",
    "#                     encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "#                 encoder_hidden_states = torch.clone(encoder_hidden_states)\n",
    "\n",
    "#                 # Predict the noise residual\n",
    "#                 noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "                                         \n",
    "#                 loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "\n",
    "#                 accelerator.backward(loss)\n",
    "#                 if accelerator.sync_gradients:\n",
    "#                     accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "                                         \n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#             # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "#             if accelerator.sync_gradients:\n",
    "#                 progress_bar.update(1)\n",
    "#                 global_step += 1\n",
    "\n",
    "#             logs = {\"loss\": loss.detach().item(),\n",
    "#                     'lr': optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "#                     'step': global_step,\n",
    "#                     'epoch': epoch+1}\n",
    "            \n",
    "#             progress_bar.set_postfix(**logs)\n",
    "#             accelerator.log(logs, step=global_step)\n",
    "\n",
    "#             if global_step >= args.max_train_steps:\n",
    "#                 break\n",
    "\n",
    "#         accelerator.wait_for_everyone()\n",
    "        \n",
    "#         # Generate sample images for visual inspection\n",
    "#         if accelerator.is_main_process:\n",
    "#             if (epoch+1) % 20 == 0:\n",
    "#                 print(epoch+1)\n",
    "#                 pipeline = StableDiffusionPipeline(\n",
    "#                     text_encoder=text_encoder,\n",
    "#                     vae=vae,\n",
    "#                     unet=accelerator.unwrap_model(unet),\n",
    "#                     tokenizer=tokenizer,\n",
    "#                     scheduler=inference_scheduler,\n",
    "#                     safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "#                     feature_extractor=feature_extractor,\n",
    "#                 ).to(accelerator.device)\n",
    "\n",
    "#                 prompt = f\"a photo of {unique_id} {class_type} with a Santa hat in the snowny forest.\"\n",
    "#                 guidance_scale = 7\n",
    "#                 generator = torch.Generator(device=pipeline.device).manual_seed(args.seed)\n",
    "                \n",
    "#                 # run pipeline in inference (sample random noise and denoise)\n",
    "#                 sample_images = []\n",
    "#                 for _ in range(args.eval_batch_size):\n",
    "#                     images = pipeline(\n",
    "#                         prompt,\n",
    "#                         guidance_scale=guidance_scale,\n",
    "#                         generator=generator).images\n",
    "#                     sample_images.extend(images)\n",
    "\n",
    "#                 accelerator.trackers[0].log({\"examples\": [wandb.Image(image) for image in sample_images]})\n",
    "#                 # accelerator.trackers[0].writer.add_images(\n",
    "#                 #     \"test_samples\", images_processed.transpose(0, 3, 1, 2), epoch\n",
    "#                 # )\n",
    "\n",
    "#     # Create the pipeline using using the trained modules and save it.\n",
    "#     if accelerator.is_main_process:\n",
    "#         print(f\"Loading pipeline and saving to {args.output_dir}...\")\n",
    "                                         \n",
    "#         scheduler = PNDMScheduler(\n",
    "#             beta_start=0.00085,\n",
    "#             beta_end=0.012,\n",
    "#             beta_schedule=\"scaled_linear\",\n",
    "#             skip_prk_steps=True,\n",
    "#             steps_offset=1,\n",
    "#         )\n",
    "                                         \n",
    "#         pipeline = StableDiffusionPipeline(\n",
    "#             text_encoder=text_encoder,\n",
    "#             vae=vae,\n",
    "#             unet=accelerator.unwrap_model(unet),\n",
    "#             tokenizer=tokenizer,\n",
    "#             scheduler=inference_scheduler,\n",
    "#             safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "#             feature_extractor=feature_extractor,\n",
    "#         )\n",
    "\n",
    "#         pipeline.save_pretrained(args.output_dir)\n",
    "        \n",
    "#     accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02a787-96d1-44ba-9df2-6e3c172498ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(text_encoder, vae, unet):\n",
    "    \n",
    "    # init Accelerator\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                              log_with=\"wandb\",\n",
    "                              logging_dir=os.path.join(args.output_dir, 'logs'))\n",
    "    \n",
    "    # set seed\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # enable gradient checkpointing\n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "        \n",
    "    # optimizer\n",
    "    if args.use_8bit_adam:\n",
    "        import bitsandbytes as bnb\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "    \n",
    "    optimizer = optimizer_class(params=unet.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    # init noise scheduler\n",
    "    noise_scheduler = DDPMScheduler(beta_start=8.5e-4,\n",
    "                                    beta_end=1.2e-2,\n",
    "                                    beta_schedule='scaled_linear',\n",
    "                                    num_train_timesteps=1000)\n",
    "    \n",
    "    # init inference scheduler\n",
    "    inference_scheduler = PNDMScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        skip_prk_steps=True,\n",
    "        steps_offset=1,\n",
    "    )\n",
    "    \n",
    "    # define train dataloader\n",
    "    train_dl = DataLoader(dataset=args.train_dataset,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=True)\n",
    "    \n",
    "    # prepare unet, optimizer and train dataloader to work with Accelerate\n",
    "    unet, optimizer, train_dl = accelerator.prepare(unet, optimizer, train_dl)\n",
    "    \n",
    "    # move text encoder and VAE to GPUs\n",
    "    text_encoder.to(accelerator.device)\n",
    "    vae.to(accelerator.device)\n",
    "    \n",
    "    # calculate the total number of epochs based on the number of training steps adjusted to grad. acc. steps\n",
    "    train_steps_per_epoch = math.ceil(len(train_dl) / args.gradient_accumulation_steps)\n",
    "    train_epochs = math.ceil(args.max_train_steps / train_steps_per_epoch)\n",
    "    \n",
    "    # total batch size\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "    \n",
    "    # init tracker\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(project_name=args.project_name, config=args)\n",
    "    \n",
    "    # init progress bar\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=(not accelerator.is_local_main_process))\n",
    "    progress_bar.set_description('Steps')\n",
    "    global_step = 0\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(train_epochs):\n",
    "        unet.train()\n",
    "        for i, batch in enumerate(train_dl):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # compute latents (i.e. image embeddings)\n",
    "                with torch.inference_mode():\n",
    "                    latents = vae.encode(batch['pixel_values']).latent_dist.sample()\n",
    "                    latents *= 0.18215\n",
    "\n",
    "                # init noise\n",
    "                noise = torch.randn_like(latents).to(accelerator.device)\n",
    "\n",
    "                # compute random timesteps\n",
    "                batch_size = latents.shape[0]\n",
    "                num_train_ts = noise_scheduler.num_train_timesteps\n",
    "                timesteps = torch.tensor([random.randint(0, num_train_ts-1) for _ in range(batch_size)])\n",
    "                timesteps = timesteps.to(accelerator.device)\n",
    "\n",
    "                # add noise to latents\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # compute text embeddings\n",
    "                with torch.inference_mode():\n",
    "                    text_embeds = text_encoder(batch['input_ids'])[0]\n",
    "                text_embeds = torch.clone(text_embeds)\n",
    "\n",
    "                # predict noise (forward pass)\n",
    "                noise_pred = unet(noisy_latents, timesteps, text_embeds).sample\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.mse_loss(noise_pred, noise, reduction='none').mean([1, 2, 3]).mean()\n",
    "\n",
    "                # backward pass\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "\n",
    "                # optimizer step & zero grad\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item(),\n",
    "                    'lr': optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch+1}\n",
    "            \n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "                \n",
    "        accelerator.wait_for_everyone()\n",
    "        \n",
    "        # Generate sample images for visual inspection\n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch+1) % 20 == 0:\n",
    "                print(f'[INFO] Generating samples for Epoch {epoch+1}...')\n",
    "                \n",
    "                pipeline = StableDiffusionPipeline(\n",
    "                    text_encoder=text_encoder,\n",
    "                    vae=vae,\n",
    "                    unet=accelerator.unwrap_model(unet),\n",
    "                    tokenizer=tokenizer,\n",
    "                    scheduler=inference_scheduler,\n",
    "                    safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "                    feature_extractor=feature_extractor,\n",
    "                ).to(accelerator.device)\n",
    "\n",
    "                prompt = f\"a photo of {unique_id} {class_type} with a Santa hat in the snowny forest.\"\n",
    "                guidance_scale = 7\n",
    "                generator = torch.Generator(device=pipeline.device).manual_seed(args.seed)\n",
    "                \n",
    "                # run pipeline in inference (sample random noise and denoise)\n",
    "                sample_images = []\n",
    "                for _ in range(args.eval_batch_size):\n",
    "                    images = pipeline(\n",
    "                        prompt,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        generator=generator).images\n",
    "                    sample_images.extend(images)\n",
    "\n",
    "                accelerator.trackers[0].log({\"examples\": [wandb.Image(image) for image in sample_images]})\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(f'Loading pipeline and saving it to {args.output_dir}')\n",
    "        \n",
    "        pipeline = StableDiffusionPipeline(\n",
    "            text_encoder=text_encoder,\n",
    "            vae=vae,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=inference_scheduler,\n",
    "            feature_extractor=feature_extractor,\n",
    "            safety_checker=StableDiffusionSafetyChecker.from_pretrained('CompVis/stable-diffusion-safety-checker')\n",
    "        )\n",
    "        \n",
    "        pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464a431-ebc1-4d41-9cf5-ab5b4c8b2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a36d90-eb7f-4946-bcce-ff90745df30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# init model\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_ckpt, subfolder='unet')\n",
    "\n",
    "num_of_gpus = 1\n",
    "notebook_launcher(train_fn, args=(text_encoder, vae, unet), num_processes=num_of_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f9dff-6a31-4cc1-bf56-6403a53b027d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f54c9-ef2b-464b-9410-23c0873fbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(args.output_dir, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6164d92-581c-4af7-a8ab-d71894d0334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"a photo of {unique_id} {class_type} with a Santa hat in the snowny forest.\"\n",
    "# negative_prompt = 'low contrast, blurry, low resolution, warped'\n",
    "guidance_scale = 7\n",
    "\n",
    "all_images = []\n",
    "num_cols = 4\n",
    "generator = torch.Generator(device=device).manual_seed(args.seed)\n",
    "\n",
    "for _ in range(num_cols):\n",
    "    images = pipeline(prompt,\n",
    "                      # negative_prompt=negative_prompt,\n",
    "                      guidance_scale=guidance_scale,\n",
    "                      generator=generator).images\n",
    "    all_images.extend(images)\n",
    "\n",
    "image_grid(all_images, 1, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cc134-b7f1-47a0-b715-d558952c3d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
