{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b69b39-ca4c-4650-a042-fbba2a578094",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071cae0-c686-4949-849c-b0185db1df08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq diffusers transformers datasets accelerate ftfy bitsandbytes==0.35.0 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3206ba-fb13-4755-9a43-34b35259f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import List, Dict\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel\n",
    "from argparse import Namespace\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "import itertools\n",
    "import datetime\n",
    "import pytz\n",
    "import PIL\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# set devie-agnostic code\n",
    "device = (\n",
    "    'mps' if torch.backends.mps.is_available()\n",
    "    else 'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "# covert PIL image to tensors\n",
    "tensor2pil = transforms.ToPILImage()\n",
    "\n",
    "# log in to WANDB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07594293-5dc3-47ac-8db1-0c232e34d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(images, rows, cols):\n",
    "    '''helper function to assample a list of PIL images in a grid'''\n",
    "    \n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f870-b181-4da6-8f95-d9139d4f8198",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28cb24-b187-4fb4-a75b-ca1dd316e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'matteopilotto/kratos'\n",
    "dataset = load_dataset(dataset_name, split='train')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d55662-3ff7-4b6d-8388-15187107dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize images\n",
    "\n",
    "image_grid([image.resize((256, 256)) for image in dataset['image']], rows=1, cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c70ad4-d6cf-4627-803b-bb54b7723542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unique token and class type to represent new concept to introduce with Dreambooth\n",
    "\n",
    "unique_token = 'krts'\n",
    "class_type = 'person'\n",
    "instance_prompt = f'a photo of {unique_token} {class_type}'\n",
    "\n",
    "print(instance_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb28a5-dd5b-4721-bc87-43f03b9d4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "define custom dataset from Dreambooth.\n",
    "Compare to the version shared in the hackathon notebook, center-cropping has been remove\n",
    "since the images in the kratos dataset are already center-cropped.\n",
    "'''\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(self, dataset, instance_prompt, tokenizer, img_size=512):\n",
    "        self.dataset = dataset\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.img_size = img_size\n",
    "        \n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            # transforms.CenterCrop(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        image = self.dataset[index]['image']\n",
    "        example['instance_image'] = self.img_transforms(image)\n",
    "        example['instance_input_ids'] = self.tokenizer(\n",
    "                                            self.instance_prompt,\n",
    "                                            padding='do_not_pad',\n",
    "                                            truncation=True,\n",
    "                                            max_length=self.tokenizer.model_max_length\n",
    "                                        ).input_ids\n",
    "        \n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af801071-0385-4cdd-a369-718425908a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "sd_ckpt = 'CompVis/stable-diffusion-v1-4'\n",
    "tokenizer = CLIPTokenizer.from_pretrained(sd_ckpt, subfolder='tokenizer')\n",
    "\n",
    "train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer, img_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280e9e1-def0-4d35-ac3b-ba06ad4da1ba",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991321ee-62a1-4ad8-aa87-8f93729b7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: List[Dict[str, Tensor]]) -> Dict[str, List[Tensor]]:\n",
    "    input_ids = [example['instance_input_ids'] for example in examples]\n",
    "    pixel_values = [example['instance_image'] for example in examples]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = tokenizer.pad({'input_ids': input_ids}, padding=True, return_tensors='pt').input_ids\n",
    "    \n",
    "    batch = {'input_ids': input_ids,\n",
    "             'pixel_values': pixel_values}\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc2961-8c5c-40eb-8567-8b5db72f9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [train_dataset[i] for i in range(len(train_dataset))]\n",
    "batch = collate_fn(examples)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab99b7-c09a-4e06-b53c-c2dcafbd4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in batch['pixel_values']:\n",
    "    display(tensor2pil(image).resize((256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720309a-93f4-40ea-bf87-6fdd0d84e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(sd_ckpt, subfolder='text_encoder')\n",
    "vae = AutoencoderKL.from_pretrained(sd_ckpt, subfolder='vae')\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_ckpt, subfolder='unet')\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained('openai/clip-vit-base-patch32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ce3f5-d65d-4681-82b9-4bbece2b3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dl = DataLoader(dataset=train_dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4bd841-4627-40e1-850a-63c7e169157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68131fc9-5c80-438f-8550-51000acb8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72e8f0-833c-4b8b-8aef-a5da9629cdec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tuning with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556acb06-4aab-4a03-aff1-396bcecd98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-6\n",
    "max_train_steps = 880\n",
    "\n",
    "# define sample prompts to visually assess the model during tranining\n",
    "sample_prompts = [\n",
    "    f'an illustration of {unique_token} {class_type} wearing a Santa hat in a snowy forest',\n",
    "    f'a photograph of {unique_token} {class_type} reading a book on the beach',\n",
    "    f'a sketch of {unique_token} {class_type} painting a mountain landscape',\n",
    "    f'a drawing of {unique_token} {class_type} wearing a Spider-man costume in the style of Marvel comics'\n",
    "]\n",
    "\n",
    "# define training hyperparamters\n",
    "args = Namespace(\n",
    "    pretrained_model_name_or_path=sd_ckpt,\n",
    "    resolution=512, # Reduce this if you want to save some memory\n",
    "    train_dataset=train_dataset,\n",
    "    instance_prompt=instance_prompt,\n",
    "    learning_rate=lr,\n",
    "    max_train_steps=max_train_steps,\n",
    "    train_batch_size=1,\n",
    "    gradient_accumulation_steps=1, # Increase this if you want to lower memory usage\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,  # set this to True to lower the memory usage.\n",
    "    use_8bit_adam=True,  # use 8bit optimizer from bitsandbytes\n",
    "    seed=2077,\n",
    "    eval_batch_size=1,\n",
    "    output_dir=\"my-dreambooth\", # where to save the pipeline\n",
    "    project_name='dreambooth-kratos',\n",
    "    guidance_scale=7,\n",
    "    train_text_encoder=True, # set to True to also fine-tune the text encoder\n",
    "    eval_samples_every_epochs=5,\n",
    "    sample_prompts=sample_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02a787-96d1-44ba-9df2-6e3c172498ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(text_encoder, vae, unet):\n",
    "    \n",
    "    # init Accelerator\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                              log_with=\"wandb\",\n",
    "                              logging_dir=os.path.join(args.output_dir, 'logs'))\n",
    "    \n",
    "    # set seed for reproducibility\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # disable text encoder fine-tuning\n",
    "    if not args.train_text_encoder:\n",
    "        text_encoder.requires_grad_(False)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    \n",
    "    vae.requires_grad_(False)\n",
    "    vae.to(accelerator.device)\n",
    "    \n",
    "    feature_extractor = CLIPFeatureExtractor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "    \n",
    "    # enable gradient checkpointing\n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "        \n",
    "    # optimizer\n",
    "    if args.use_8bit_adam:\n",
    "        import bitsandbytes as bnb\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "    \n",
    "    # collate together models' parameters for optimization\n",
    "    # ref. itertools.chain: https://discuss.pytorch.org/t/giving-multiple-parameters-in-optimizer/869/8?u=matteo_pilotto\n",
    "    if args.train_text_encoder:\n",
    "        params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "    else:\n",
    "        params_to_optimize = unet.parameters()\n",
    "        \n",
    "    optimizer = optimizer_class(params=params_to_optimize, lr=args.learning_rate)\n",
    "    \n",
    "    # init noise scheduler\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        beta_start=8.5e-4,\n",
    "        beta_end=1.2e-2,\n",
    "        beta_schedule='scaled_linear',\n",
    "        num_train_timesteps=1000\n",
    "    )\n",
    "    \n",
    "    # init inference scheduler\n",
    "    inference_scheduler = PNDMScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        skip_prk_steps=True,\n",
    "        steps_offset=1,\n",
    "    )\n",
    "    \n",
    "    # define train dataloader\n",
    "    train_dl = DataLoader(dataset=args.train_dataset,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=True)\n",
    "    \n",
    "    # calculate the total number of epochs based on the number of training steps adjusted to grad. acc. steps\n",
    "    train_steps_per_epoch = math.ceil(len(train_dl) / args.gradient_accumulation_steps)\n",
    "    train_epochs = math.ceil(args.max_train_steps / train_steps_per_epoch)\n",
    "    \n",
    "    # total batch size\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "    \n",
    "    # prepare unet, optimizer and train dataloader to work with Accelerate\n",
    "    if args.train_text_encoder:\n",
    "        unet, text_encoder, optimizer, train_dl = accelerator.prepare(unet, text_encoder, optimizer, train_dl)\n",
    "    else:\n",
    "        unet, optimizer, train_dl = accelerator.prepare(unet, optimizer, train_dl)\n",
    "    \n",
    "    # init tracker\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(project_name=args.project_name, config=args)\n",
    "    \n",
    "    # init WANDB table to collect and save sample outputs during training\n",
    "    log_prompts = [prompt for prompt in args.sample_prompts for i in range(args.eval_batch_size)]\n",
    "    log_table = wandb.Table(columns=['prompt'])\n",
    "    for prompt in log_prompts:\n",
    "        log_table.add_data(prompt)\n",
    "    \n",
    "    # init progress bar\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=(not accelerator.is_local_main_process))\n",
    "    progress_bar.set_description('Steps')\n",
    "    global_step = 0\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(train_epochs):\n",
    "        unet.train()\n",
    "        \n",
    "        if args.train_text_encoder:\n",
    "            text_encoder.train()\n",
    "            \n",
    "        for i, batch in enumerate(train_dl):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # compute latents (i.e. image embeddings)\n",
    "                with torch.inference_mode():\n",
    "                    latents = vae.encode(batch['pixel_values']).latent_dist.sample()\n",
    "                    latents *= 0.18215\n",
    "\n",
    "                # init noise\n",
    "                noise = torch.randn_like(latents).to(accelerator.device)\n",
    "\n",
    "                # compute random timesteps\n",
    "                batch_size = latents.shape[0]\n",
    "                num_train_ts = noise_scheduler.num_train_timesteps\n",
    "                timesteps = torch.tensor([random.randint(0, num_train_ts-1) for _ in range(batch_size)])\n",
    "                timesteps = timesteps.to(accelerator.device)\n",
    "\n",
    "                # add noise to latents\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # compute text embeddings\n",
    "                # with torch.inference_mode():\n",
    "                #     text_embeds = text_encoder(batch['input_ids'])[0]\n",
    "                # text_embeds = torch.clone(text_embeds)\n",
    "                text_embeds = text_encoder(batch['input_ids'])[0]\n",
    "                \n",
    "\n",
    "                # predict noise (forward pass)\n",
    "                noise_pred = unet(noisy_latents, timesteps, text_embeds).sample\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.mse_loss(noise_pred, noise, reduction='none').mean([1, 2, 3]).mean()\n",
    "\n",
    "                # backward pass\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    if args.train_text_encoder:\n",
    "                        params_to_clip = itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "                    else:\n",
    "                        params_to_clip = unet.parameters()\n",
    "                        \n",
    "                accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "\n",
    "                # optimizer step & zero grad\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item(),\n",
    "                    'lr': optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch+1}\n",
    "            \n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "                \n",
    "        accelerator.wait_for_everyone()\n",
    "        \n",
    "        # Generate sample images for visual inspection\n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch+1) % args.eval_samples_every_epochs == 0:\n",
    "                print(f'[INFO] Generating samples for Epoch {epoch+1}...')\n",
    "                \n",
    "                pipeline = StableDiffusionPipeline(\n",
    "                    text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "                    vae=vae,\n",
    "                    unet=accelerator.unwrap_model(unet),\n",
    "                    tokenizer=tokenizer,\n",
    "                    scheduler=inference_scheduler,\n",
    "                    safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "                    feature_extractor=feature_extractor,\n",
    "                ).to(accelerator.device)\n",
    "                \n",
    "                sample_images = []\n",
    "                for sample_prompt in sample_prompts:\n",
    "                    generator = torch.Generator(device=pipeline.device).manual_seed(args.seed)\n",
    "                \n",
    "                    # run pipeline in inference (sample random noise and denoise)\n",
    "                    for _ in range(args.eval_batch_size):\n",
    "                        print(f'[INFO] generating image for \"{sample_prompt}\" prompt...')\n",
    "                        images = pipeline(\n",
    "                            prompt=sample_prompt,\n",
    "                            guidance_scale=args.guidance_scale,\n",
    "                            generator=generator).images\n",
    "                        \n",
    "                        sample_images.extend(images)\n",
    "\n",
    "                log_images = [wandb.Image(image) for image in sample_images]\n",
    "                accelerator.trackers[0].log({\"sample images\": log_images})\n",
    "                log_table.add_column(name=f'epoch_{epoch+1}', data=log_images)\n",
    "    \n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.trackers[0].log({'table': log_table})\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(f'[INFO] Loading pipeline and saving it to {args.output_dir}')\n",
    "        \n",
    "        pipeline = StableDiffusionPipeline(\n",
    "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "            vae=vae,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=inference_scheduler,\n",
    "            feature_extractor=feature_extractor,\n",
    "            safety_checker=StableDiffusionSafetyChecker.from_pretrained('CompVis/stable-diffusion-safety-checker')\n",
    "        )\n",
    "        \n",
    "        pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a36d90-eb7f-4946-bcce-ff90745df30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# init stable diffusion components\n",
    "text_encoder = CLIPTextModel.from_pretrained(sd_ckpt, subfolder='text_encoder')\n",
    "vae = AutoencoderKL.from_pretrained(sd_ckpt, subfolder='vae')\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_ckpt, subfolder='unet')\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "num_of_gpus = 1\n",
    "notebook_launcher(train_fn, args=(text_encoder, vae, unet), num_processes=num_of_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca942b-4f11-440d-9cc1-b39bc8b749df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save model to WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac3e8c-ddbc-4b14-ac40-689be14de821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL'S CHECKPOINT ON WANDB \n",
    "# ref: https://docs.wandb.ai/guides/track/advanced/resuming\n",
    "# ref: https://docs.wandb.ai/ref/python/artifact\n",
    "\n",
    "PROJECT_NAME = 'dreambooth-kratos'\n",
    "RUN_ID = 'ay51iqly'\n",
    "MODEL_PATH = f'./my-dreambooth'\n",
    "\n",
    "with wandb.init(project=PROJECT_NAME, id=RUN_ID, resume=True) as run:\n",
    "    artifact = wandb.Artifact(RUN_ID, type='model')\n",
    "    artifact.add_dir(MODEL_PATH)\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e60108-9cd1-405d-a1de-715ba018ec4f",
   "metadata": {},
   "source": [
    "### Download model from WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533c1b8-0295-4633-b086-e88d1ae24ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD MODEL'S CHECKPOINT FROM WANDB\n",
    "\n",
    "PROJECT_NAME = 'dreambooth-kratos'\n",
    "RUN_ID = 'ay51iqly'\n",
    "with wandb.init(project=PROJECT_NAME, id=RUN_ID, resume=True) as run:\n",
    "    artifact = wandb.use_artifact(f'matt24/{PROJECT_NAME}/{RUN_ID}:v0', type='model')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "print(f'[INFO] Artifact directory: {artifact_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f9dff-6a31-4cc1-bf56-6403a53b027d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f54c9-ef2b-464b-9410-23c0873fbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_wandb = True\n",
    "\n",
    "if load_from_wandb:\n",
    "    print('[INFO] Loading fine-tuned model from WANDB...')\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(artifact_dir, torch_dtype=torch.float16).to(device)\n",
    "else:\n",
    "    print('[INFO] Load fine-tuned model from local directory...')\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(args.output_dir, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6164d92-581c-4af7-a8ab-d71894d0334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images to generate\n",
    "num_samples = 16\n",
    "\n",
    "prompt = f\"An illustration of {unique_token} {class_type}num_samples punk playing electric guitar, tristan eaton, victo ngai, artgerm, rhads, ross draws, 4k\"\n",
    "negative_prompt = 'low contrast, blurry, low resolution, warped'\n",
    "use_negative_prompt = True\n",
    "guidance_scale = 7\n",
    "h = 512\n",
    "w = 512\n",
    "num_inference_steps = 50\n",
    "\n",
    "\n",
    "# wdefine variables to log samples outputs to WANDB\n",
    "PROJECT_NAME = 'dreambooth-kratos'\n",
    "RUN_ID = 'ay51iqly'\n",
    "inference_table_name = 'inference_table'\n",
    "log_table_cols = [\n",
    "    'image',\n",
    "    'prompt',\n",
    "    'negative prompt',\n",
    "    'resolution',\n",
    "    'guidance scale',\n",
    "    'inference steps',\n",
    "    'seed'\n",
    "]\n",
    "\n",
    "# keep track of time\n",
    "current_time = datetime.datetime.now(pytz.timezone('Europe/Amsterdam')).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# initialize WANDB artficat and table\n",
    "inference_artifact = wandb.Artifact(f'inference_artifact_{current_time}', type='inference') \n",
    "inference_log_table = wandb.Table(columns=log_table_cols)\n",
    "\n",
    "# collect outputs\n",
    "all_images = []\n",
    "all_seeds = []\n",
    "\n",
    "\n",
    "\n",
    "# generate images\n",
    "for _ in tqdm(range(num_samples)):\n",
    "    seed = random.randint(1, 1000000)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt,\n",
    "                      negative_prompt=negative_prompt if use_negative_prompt else None,\n",
    "                      guidance_scale=guidance_scale,\n",
    "                      height=h,\n",
    "                      width=w,\n",
    "                      num_inference_steps=num_inference_steps,\n",
    "                      generator=generator\n",
    "                     ).images[0]\n",
    "    \n",
    "    inference_log_table.add_data(\n",
    "        wandb.Image(image),\n",
    "        prompt,\n",
    "        negative_prompt if use_negative_prompt else None,\n",
    "        f'{w} x {h}',\n",
    "        guidance_scale,\n",
    "        num_inference_steps,\n",
    "        seed\n",
    "    )\n",
    "    \n",
    "    all_images.append(image)\n",
    "    all_seeds.append(seed)\n",
    "\n",
    "inference_artifact.add(inference_log_table, 'inference_table')\n",
    "\n",
    "\n",
    "with wandb.init(project=PROJECT_NAME, id=RUN_ID, resume=True) as run:\n",
    "    print(f'[ðŸ’¡INFO] Saving dato to {inference_table_name}_{current_time}...')\n",
    "    run.log_artifact(inference_artifact)\n",
    "    \n",
    "    \n",
    "image_grid([image.resize((256, 256)) for image in all_images], 1, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacb4f5-5e65-4c36-8810-edc2a599dec7",
   "metadata": {},
   "source": [
    "### Push on the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1693e3-914e-4327-931f-282e47af3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2d78b-75cc-48cf-afde-74f7534f1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt -qq install git-lfs\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475085f3-f607-4355-af8d-f3768bc34453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a name for your model on the Hub. No spaces allowed.\n",
    "model_name = f'{unique_token}_{class_type}'\n",
    "print(model_name)\n",
    "\n",
    "\n",
    "theme = 'wildcard'\n",
    "\n",
    "# Describe the theme and model you've trained\n",
    "description = f\"\"\"\n",
    "This is a Stable Diffusion model fine-tuned on imagaes of Kratos from God of War for the {theme} theme using `CompVis/stable-diffusion-v1-4` pre-trained model.\n",
    "\"\"\"\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b658a10-567f-4e46-a566-a38176a13358",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ./artifacts/ay51iqly:v0 ./my-dreambooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d0bf0-55dc-4398-80ae-5a146c924003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to upload a pipeline saved locally to the hub\n",
    "from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name\n",
    "\n",
    "# Set up repo and upload files\n",
    "hub_model_id = get_full_repo_name(model_name)\n",
    "create_repo(hub_model_id)\n",
    "api = HfApi()\n",
    "api.upload_folder(folder_path=args.output_dir, path_in_repo=\"\", repo_id=hub_model_id)\n",
    "\n",
    "content = f\"\"\"\n",
    "---\n",
    "license: creativeml-openrail-m\n",
    "tags:\n",
    "- pytorch\n",
    "- diffusers\n",
    "- stable-diffusion\n",
    "- text-to-image\n",
    "- diffusion-models-class\n",
    "- dreambooth-hackathon\n",
    "- {theme}\n",
    "---\n",
    "\n",
    "# DreamBooth model for the {name_of_your_concept} concept trained by {api.whoami()[\"name\"]} on the {dataset_id} dataset.\n",
    "\n",
    "This is a Stable Diffusion model fine-tuned on the {name_of_your_concept} concept with DreamBooth. It can be used by modifying the `instance_prompt`: **{instance_prompt}**\n",
    "\n",
    "This model was created as part of the DreamBooth Hackathon ðŸ”¥. Visit the [organisation page](https://huggingface.co/dreambooth-hackathon) for instructions on how to take part!\n",
    "\n",
    "## Description\n",
    "\n",
    "{description}\n",
    "\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained('{hub_model_id}')\n",
    "image = pipeline().images[0]\n",
    "image\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(content)\n",
    "hub_url = card.push_to_hub(hub_model_id)\n",
    "print(f\"Upload successful! Model can be found here: {hub_url}\")\n",
    "print(\n",
    "    f\"View your submission on the public leaderboard here: https://huggingface.co/spaces/dreambooth-hackathon/leaderboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c7b05-2ade-49ae-84cc-50316b1c4390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
